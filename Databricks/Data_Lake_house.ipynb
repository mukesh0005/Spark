{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Lake House\n",
    "- A Data Lakehouse is a hybrid data architecture that combines the flexibility and scalability of a data lake with the performance and governance features of a data warehouse. \n",
    "- It enables organizations to run BI, machine learning (ML), and real-time analytics on a single platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features\n",
    "- structured, semi-structured, and unstructured data\n",
    "- ACID \n",
    "- Schema enforcement\n",
    "- Time Travel & Data Versioning\n",
    "- Batch & stream workloads\n",
    "* The underlying storage layer is cloud object storage only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Databricks uses Delta live tables\n",
    "#### Snowflake uses ApacheIceberg or Aws athena\n",
    "- with help of apache iceberg snowflake directly queries from data lake storage with out ingesting into snowflake tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delta Lake\n",
    "- stores data in parquet format and columnar \n",
    "- Delta Lake adds a transaction log (_delta_log) that tracks all changes to the data and ensures ACID compliance.\n",
    "- since ACID schema enforcement\n",
    "- time travel is available with transaction log various versions of data is saved.\n",
    "- manages infrastructure at scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declarative vs procedural \n",
    "- built in data quality checks\n",
    "- automates medallion architecture\n",
    "- monitoring and lineage tracking\n",
    "- automatic performance optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components\n",
    "- Delta table - a table that stores all of the data.\n",
    "- Delta log -  transaction log \n",
    "- Delta Cache - transaction cache which stores recent versions of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Upserts are done via MERGE or INSERT INTO\n",
    "- checkpoints are used for recovery\n",
    "- snapshots for rollback \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "--- Live Table creation syntax\n",
    "CREATE LIVE TABLE silver_sales\n",
    "AS SELECT * FROM STREAM(live.bronze_sales)\n",
    "WHERE EXPECT(amount > 0, \"Transaction amount must be positive\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- read from another live table\n",
    "Create or refresh LIVE table top_five \n",
    "As select * from live.silver_sales\n",
    "limit 5;\n",
    "-- a key word temporary can be added to create temporary tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLT pipeline via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create json pay loads\n",
    "pipeline_payload = {\n",
    "  \"name\": \"DLT_Pipeline_API\",\n",
    "  \"storage\": \"dbfs:/pipelines/dlt_pipeline_api\", ##storage for pipeline logs \n",
    "  \"target\": \"dlt_target_db\", ## target db for pipeline\n",
    "  \"development\": false,\n",
    "  \"clusters\": [\n",
    "    {\n",
    "      \"label\": \"default\",\n",
    "      \"num_workers\": 2\n",
    "    }\n",
    "  ],\n",
    "  \"libraries\": [\n",
    "    {\n",
    "      \"notebook\": {\n",
    "        \"path\": \"/Repos/your_repo/dlt_notebook\" ##notebook path for transformations\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"edition\": \"ADVANCED\",\n",
    "  \"photon\": true,\n",
    "  \"continuous\": true # can be triggered\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the DLT pipeline Using API \n",
    "curl -X POST https://<DATABRICKS_WORKSPACE>/api/2.0/pipelines \\\n",
    "-H \"Authorization: Bearer <DATABRICKS_TOKEN>\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d @pipeline_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Databricks API credentials\n",
    "DATABRICKS_HOST = \"https://<DATABRICKS_WORKSPACE>\"\n",
    "TOKEN = \"<DATABRICKS_TOKEN>\"\n",
    "\n",
    "# Pipeline Configuration\n",
    "pipeline_config = {\n",
    "    \"name\": \"DLT_Pipeline_API\",\n",
    "    \"storage\": \"dbfs:/pipelines/dlt_pipeline_api\",\n",
    "    \"target\": \"dlt_target_db\",\n",
    "    \"development\": False,\n",
    "    \"clusters\": [{\"label\": \"default\", \"num_workers\": 2}],\n",
    "    \"libraries\": [{\"notebook\": {\"path\": \"/Repos/your_repo/dlt_notebook\"}}],\n",
    "    \"edition\": \"ADVANCED\",\n",
    "    \"photon\": True,\n",
    "    \"continuous\": True\n",
    "}\n",
    "\n",
    "# API Call\n",
    "headers = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "response = requests.post(f\"{DATABRICKS_HOST}/api/2.0/pipelines\", headers=headers, data=json.dumps(pipeline_config))\n",
    "\n",
    "# Print response\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the response contains the pipeline id which can be referred for starting and monitoring\n",
    "- we can curl or python to get the list of all the pipelines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## start the pipeline \n",
    "curl -X POST https://<DATABRICKS_WORKSPACE>/api/2.0/pipelines/<PIPELINE_ID>/start \\\n",
    "-H \"Authorization: Bearer <DATABRICKS_TOKEN>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## status\n",
    "curl -X GET https://<DATABRICKS_WORKSPACE>/api/2.0/pipelines/<PIPELINE_ID>/status \\\n",
    "-H \"Authorization: Bearer <DATABRICKS_TOKEN>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- create eventlog using pipeline id\n",
    "CREATE VIEW event_log_raw AS SELECT * FROM event_log(\"<pipeline-ID>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Query lineage informations\n",
    " SELECT\n",
    "  details:flow_definition.output_dataset as output_dataset,\n",
    "  details:flow_definition.input_datasets as input_dataset\n",
    " FROM\n",
    "  event_log_raw\n",
    " WHERE\n",
    "  event_type = 'flow_definition'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Query data quality information\n",
    "SELECT\n",
    "  details:flow_progress.data_quality.expectations\n",
    " FROM\n",
    "  event_log_raw  ---> event log table in unity catalog\n",
    " WHERE\n",
    "  event_type = 'flow_progress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- get back log data \n",
    " SELECT\n",
    " timestamp,\n",
    " Double(details :flow_progress.metrics.backlog_bytes) as backlog\n",
    " FROM\n",
    "  event_log_raw\n",
    " WHERE\n",
    "  event_type ='flow_progress'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- when autoscaling is enabled cluster resize is done this info is also stored \n",
    " SELECT\n",
    " details:autoscale \n",
    " FROM\n",
    "  event_log_raw\n",
    " WHERE\n",
    "  event_type ='autoscale'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- check for cluster resources \n",
    "SELECT\n",
    " timestamp,\n",
    " Double(details :cluster_resources.avg_num_queued_tasks) as queue_size\n",
    " FROM\n",
    "  event_log_raw\n",
    " WHERE\n",
    "  event_type = 'cluster_resources';\n",
    "\n",
    "-- avg_task_slot_utilization\n",
    "-- num_executors\n",
    "-- latest_requested_num_executors\n",
    "-- optimal_num_executors\n",
    "-- state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- user actions\n",
    "SELECT timestamp, details:user_action:action, details:user_action:user_name FROM\n",
    " event_log_raw WHERE event_type = 'user_action';\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FDE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
