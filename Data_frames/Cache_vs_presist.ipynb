{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- size of dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Convert DataFrame to Pandas DataFrame\n",
    "pandas_df = df.toPandas()\n",
    "\n",
    "# Estimate memory usage\n",
    "memory_usage_mb = sys.getsizeof(pandas_df) / (1024 * 1024)  # Convert to MB\n",
    "print(\"Estimated memory usage:\", memory_usage_mb, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partitioning\n",
    "Purpose: Partitioning divides data into logical partitions based on one or more columns, typically improving query performance by reducing the amount of data to scan.\n",
    "\n",
    "Mechanism: Spark organizes data into directories based on partition columns, making it easy to filter and retrieve data based on partition values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.partitionBy(\"year\", \"month\").parquet(\"path/to/table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucketing\n",
    "Purpose: Bucketing distributes data evenly into a fixed number of buckets based on a hash function applied to one or more columns, typically improving join and aggregation performance.\n",
    "\n",
    "Mechanism: Spark writes data into a fixed number of files, or buckets, based on the hash value of the bucketing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.bucketBy(10, \"customer_id\").sortBy(\"transaction_date\").saveAsTable(\"bucketed_table\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columnar Storage: Parquet is a columnar storage format, similar to Redshift's internal storage format. This alignment makes data loading into Redshift more efficient because it can take advantage of Redshift's optimized columnar storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"compression\", \"snappy\") \\  # Choose your preferred compression codec\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"s3://your-bucket/path/to/save/location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explode() function in PySpark is used to transform an array or map column into multiple rows, with one row for each element of the array or key-value pair of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df = df.select(\"name\", explode(\"fruits\").alias(\"fruit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coalesce() function in PySpark is used to return the first non-null value from a set of columns or expressions. It takes a variable number of arguments and returns the first argument that is not null. If all arguments are null, coalesce() returns null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_non_null_age = df.select(coalesce(col(\"age1\"), col(\"age2\")).alias(\"first_non_null_age\")).first()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_non_null_age = df.select(\n",
    "    \"name\",\n",
    "    when(col(\"country\") == \"US\", coalesce(col(\"age1\"), col(\"age2\"))). \\\n",
    "        when(col(\"country\") == \"UK\", col(\"age2\")). \\\n",
    "        when(col(\"country\").isNull(), coalesce(col(\"age1\"), col(\"age2\"))). \\\n",
    "        alias(\"first_non_null_age\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window functions in PySpark allow you to perform computations across rows of a DataFrame related to the current row, similar to SQL window functions.\n",
    "\n",
    "Partitioning: Window functions are typically applied within partitions of a DataFrame. You can partition the data by one or more columns using the partitionBy() method.\n",
    "\n",
    "Ordering: Within each partition, rows are usually ordered based on one or more columns. You can specify the ordering using the orderBy() method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Window Functions Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [(\"Alice\", 25),\n",
    "        (\"Bob\", 30),\n",
    "        (\"Cathy\", 28),\n",
    "        (\"Dave\", 35),\n",
    "        (\"Emily\", 27)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"name\", \"age\"])\n",
    "\n",
    "# Define a window specification partitioned by no columns and ordered by age in descending order\n",
    "window_spec = Window.orderBy(df[\"age\"].desc())\n",
    "\n",
    "# Add a new column \"rank\" using the row_number() window function\n",
    "df_with_rank = df.withColumn(\"rank\", row_number().over(window_spec))\n",
    "\n",
    "# Show the DataFrame with ranks\n",
    "df_with_rank.show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
