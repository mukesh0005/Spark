{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession;\n",
    "from pyspark.context import SparkContext\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Logistic_Regression\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://muki.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Logistic_Regression</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1cda2be5a10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\veera\\anaconda3\\envs\\DEV\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:485: FutureWarning: is_datetime64tz_dtype is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.DatetimeTZDtype)` instead.\n",
      "  if should_localize and is_datetime64tz_dtype(s.dtype) and s.dt.tz is not None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+--------------+\n",
      "|Attempt|Success| Bait|         Place|\n",
      "+-------+-------+-----+--------------+\n",
      "|      1|      0|Bread|Grandma's Pond|\n",
      "|      2|      1|Bread|Grandma's Pond|\n",
      "|      3|      1|Bread|Grandma's Pond|\n",
      "|      4|      1|Bread|Grandma's Pond|\n",
      "|      5|      1|Bread|Grandma's Pond|\n",
      "+-------+-------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('./data/LR.xlsx')\n",
    "df = spark.createDataFrame(df)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before building the model, we need to assemble the input features into a single feature vector using the VectorAssembler class. Then, we will split the dataset into a training set (80%) and a testing set (20%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Indexing the category variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------------+------------+-------------+\n",
      "|Attempt|Success|    Bait|            Place|Bait_indexed|Place_indexed|\n",
      "+-------+-------+--------+-----------------+------------+-------------+\n",
      "|      1|      0|   Bread|   Grandma's Pond|         0.0|          0.0|\n",
      "|      2|      1|   Bread|   Grandma's Pond|         0.0|          0.0|\n",
      "|      3|      1|   Bread|   Grandma's Pond|         0.0|          0.0|\n",
      "|      4|      1|   Bread|   Grandma's Pond|         0.0|          0.0|\n",
      "|      5|      1|   Bread|   Grandma's Pond|         0.0|          0.0|\n",
      "|      6|      0|   Bread|   Grandma's Pond|         0.0|          0.0|\n",
      "|      7|      1|Hot Dogs|   Grandma's Pond|         1.0|          0.0|\n",
      "|      8|      0|Hot Dogs|   Grandma's Pond|         1.0|          0.0|\n",
      "|      9|      1|Hot Dogs|   Grandma's Pond|         1.0|          0.0|\n",
      "|     10|      1|Hot Dogs|   Grandma's Pond|         1.0|          0.0|\n",
      "|     11|      1|Hot Dogs|   Grandma's Pond|         1.0|          0.0|\n",
      "|     12|      1|Hot Dogs|   Grandma's Pond|         1.0|          0.0|\n",
      "|     13|      0|   Bread|Uncle Ron's Canal|         0.0|          1.0|\n",
      "|     14|      0|   Bread|Uncle Ron's Canal|         0.0|          1.0|\n",
      "|     15|      0|   Bread|Uncle Ron's Canal|         0.0|          1.0|\n",
      "|     16|      0|   Bread|Uncle Ron's Canal|         0.0|          1.0|\n",
      "|     17|      0|   Bread|Uncle Ron's Canal|         0.0|          1.0|\n",
      "|     18|      1|Hot Dogs|Uncle Ron's Canal|         1.0|          1.0|\n",
      "|     19|      0|Hot Dogs|Uncle Ron's Canal|         1.0|          1.0|\n",
      "|     20|      1|Hot Dogs|Uncle Ron's Canal|         1.0|          1.0|\n",
      "+-------+-------+--------+-----------------+------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "indexed_df = df\n",
    "for category in ['Bait','Place']:\n",
    "    indexer = StringIndexer(inputCol=category, outputCol=str(category) +'_indexed')\n",
    "    indexed_df = indexer.fit(indexed_df).transform(indexed_df)\n",
    "indexed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+-----------------+------------+-------------+-------------------+--------------------+\n",
      "|Attempt|Success|    Bait|            Place|Bait_indexed|Place_indexed|Bait_indexed_onehot|Place_indexed_onehot|\n",
      "+-------+-------+--------+-----------------+------------+-------------+-------------------+--------------------+\n",
      "|      1|      0|   Bread|   Grandma's Pond|         0.0|          0.0|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      2|      1|   Bread|   Grandma's Pond|         0.0|          0.0|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      3|      1|   Bread|   Grandma's Pond|         0.0|          0.0|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      4|      1|   Bread|   Grandma's Pond|         0.0|          0.0|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      5|      1|   Bread|   Grandma's Pond|         0.0|          0.0|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      6|      0|   Bread|   Grandma's Pond|         0.0|          0.0|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      7|      1|Hot Dogs|   Grandma's Pond|         1.0|          0.0|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|      8|      0|Hot Dogs|   Grandma's Pond|         1.0|          0.0|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|      9|      1|Hot Dogs|   Grandma's Pond|         1.0|          0.0|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|     10|      1|Hot Dogs|   Grandma's Pond|         1.0|          0.0|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|     11|      1|Hot Dogs|   Grandma's Pond|         1.0|          0.0|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|     12|      1|Hot Dogs|   Grandma's Pond|         1.0|          0.0|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|     13|      0|   Bread|Uncle Ron's Canal|         0.0|          1.0|      (1,[0],[1.0])|           (1,[],[])|\n",
      "|     14|      0|   Bread|Uncle Ron's Canal|         0.0|          1.0|      (1,[0],[1.0])|           (1,[],[])|\n",
      "|     15|      0|   Bread|Uncle Ron's Canal|         0.0|          1.0|      (1,[0],[1.0])|           (1,[],[])|\n",
      "|     16|      0|   Bread|Uncle Ron's Canal|         0.0|          1.0|      (1,[0],[1.0])|           (1,[],[])|\n",
      "|     17|      0|   Bread|Uncle Ron's Canal|         0.0|          1.0|      (1,[0],[1.0])|           (1,[],[])|\n",
      "|     18|      1|Hot Dogs|Uncle Ron's Canal|         1.0|          1.0|          (1,[],[])|           (1,[],[])|\n",
      "|     19|      0|Hot Dogs|Uncle Ron's Canal|         1.0|          1.0|          (1,[],[])|           (1,[],[])|\n",
      "|     20|      1|Hot Dogs|Uncle Ron's Canal|         1.0|          1.0|          (1,[],[])|           (1,[],[])|\n",
      "+-------+-------+--------+-----------------+------------+-------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_df = indexed_df\n",
    "for category in ['Bait_indexed','Place_indexed']:\n",
    "    encoder = OneHotEncoder(inputCol=category, outputCol=str(category) +'_onehot')\n",
    "    encoded_df = encoder.fit(encoded_df).transform(encoded_df)\n",
    "encoded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+\n",
      "|Success|Bait_indexed_onehot|Place_indexed_onehot|\n",
      "+-------+-------------------+--------------------+\n",
      "|      0|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      1|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      1|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      1|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      1|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      0|      (1,[0],[1.0])|       (1,[0],[1.0])|\n",
      "|      1|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|      0|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|      1|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|      1|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|      1|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|      1|          (1,[],[])|       (1,[0],[1.0])|\n",
      "|      0|      (1,[0],[1.0])|           (1,[],[])|\n",
      "|      0|      (1,[0],[1.0])|           (1,[],[])|\n",
      "|      0|      (1,[0],[1.0])|           (1,[],[])|\n",
      "|      0|      (1,[0],[1.0])|           (1,[],[])|\n",
      "|      0|      (1,[0],[1.0])|           (1,[],[])|\n",
      "|      1|          (1,[],[])|           (1,[],[])|\n",
      "|      0|          (1,[],[])|           (1,[],[])|\n",
      "|      1|          (1,[],[])|           (1,[],[])|\n",
      "+-------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = ['Attempt','Bait','Place','Bait_indexed','Place_indexed']\n",
    "df = encoded_df\n",
    "for col in cols_to_drop:\n",
    "    df = df.drop(col)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=[\"Bait_indexed_onehot\", \"Place_indexed_onehot\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+--------------------+---------+\n",
      "|Success|Bait_indexed_onehot|Place_indexed_onehot| features|\n",
      "+-------+-------------------+--------------------+---------+\n",
      "|      0|      (1,[0],[1.0])|       (1,[0],[1.0])|[1.0,1.0]|\n",
      "|      1|      (1,[0],[1.0])|       (1,[0],[1.0])|[1.0,1.0]|\n",
      "|      1|      (1,[0],[1.0])|       (1,[0],[1.0])|[1.0,1.0]|\n",
      "|      0|      (1,[0],[1.0])|       (1,[0],[1.0])|[1.0,1.0]|\n",
      "|      0|          (1,[],[])|       (1,[0],[1.0])|[0.0,1.0]|\n",
      "+-------+-------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cls = RandomForestClassifier(numTrees=10, featuresCol=\"features\", labelCol=\"Success\")\n",
    "model = rf_cls.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy, Precision, and Recall\n",
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"Success\", predictionCol=\"prediction\")\n",
    "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
    "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333\n",
      "Precision: 0.8667\n",
      "Recall: 0.8333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boosted Trees build multiple decision trees sequentially, where each tree corrects the errors made by the previous trees.\n",
    "- another ensemble learning technique that combines multiple weak learners to create a strong learner. AdaBoost assigns weights to the training instances and adjusts these weights at each iteration to focus on misclassified instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import GBTClassifier, AdaBoostClassifier\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"GradientBoostingAndAdaBoostExample\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(0, 2.0, 1.0, 1.0), (1, 3.0, 2.0, 0.0), (0, 4.0, 3.0, 0.0), (1, 5.0, 4.0, 1.0)]\n",
    "columns = [\"label\", \"feature1\", \"feature2\", \"feature3\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Assemble features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"feature3\"], outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Train Gradient Boosted Trees classifier\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol='label', maxIter=10)\n",
    "gbt_model = gbt.fit(df)\n",
    "\n",
    "# Train AdaBoost classifier\n",
    "adaBoost = AdaBoostClassifier(featuresCol='features', labelCol='label', maxIter=10)\n",
    "adaBoost_model = adaBoost.fit(df)\n",
    "\n",
    "# Make predictions with Gradient Boosted Trees\n",
    "gbt_predictions = gbt_model.transform(df)\n",
    "gbt_predictions.select(\"features\", \"label\", \"probability\", \"prediction\").show()\n",
    "\n",
    "# Make predictions with AdaBoost\n",
    "adaBoost_predictions = adaBoost_model.transform(df)\n",
    "adaBoost_predictions.select(\"features\", \"label\", \"probability\", \"prediction\").show()\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
